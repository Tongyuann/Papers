# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
## BERT:用于语言理解的深度双向变形器预训练
## 摘要
BERT旨在通过在所有层中对左右上下文进行联合条件反射，从未标记的文本中预训练深度双向表示。。因此，预训练的BERT模型可以通过一个额外的输出层进行微调，从而为广泛的任务(如问答和语言推理)创建最先进的模型，而**无需对特定于任务的架构进行大量修改**
## 简介
1. 预训练的语言表示应用于下游任务有两种现有策略:基于特征和微调。基于特征的方法，使用特定于任务的架构，其中包括预训练的表示作为附加特征。微调方法，如生成式预训练转换器(OpenAI GPT)，引入了最小的任务特定参数，并通过简单地微调**所有**预训练参数来对下游任务进行训练。这两种方法在预训练过程中共享相同的目标函数，它们使用单向语言模型来学习通用语言表示  
2. **目前的技术限制了预训练表征的能力，特别是对于微调方法。主要的限制是标准语言模型是单向的，这限制了在预训练期间可以使用的体系结构的选择**  
3. ***提出BERT:来自变形金刚的双向编码器表示来改进基于微调的方法（Bidirectional Encoder Representations from Transformers）***
4. 运用完形填空任务的掩模算法，随机遮住句子中的一些词，目的是预测遮住的词，除了掩码语言模型，我们还使用了一个“下一句话预测”任务，联合预训练文本对表示
## 贡献
1. 证明了双向预训练对语言表示的重要性
2. 预训练的表示减少了对许多重型工程任务特定架构的需求
3. BERT在11个NLP任务中推进了最先进的技术